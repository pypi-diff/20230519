# Comparing `tmp/deep_training-0.1.5.post4-py3-none-any.whl.zip` & `tmp/deep_training-0.1.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,13 +1,13 @@
-Zip file size: 346097 bytes, number of entries: 185
+Zip file size: 346076 bytes, number of entries: 185
 -rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      903 b- defN 23-May-19 06:00 deep_training/setup.py
+-rw-rw-rw-  2.0 fat      897 b- defN 23-May-19 07:06 deep_training/setup.py
 -rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
 -rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
--rw-rw-rw-  2.0 fat    17724 b- defN 23-May-04 00:28 deep_training/data_helper/data_helper.py
+-rw-rw-rw-  2.0 fat    17834 b- defN 23-May-19 06:58 deep_training/data_helper/data_helper.py
 -rw-rw-rw-  2.0 fat     5041 b- defN 23-Apr-28 06:13 deep_training/data_helper/data_module.py
 -rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-27 00:33 deep_training/data_helper/training_args.py
 -rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
 -rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
 -rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
 -rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
@@ -80,25 +80,25 @@
 -rw-rw-rw-  2.0 fat     4209 b- defN 23-Apr-25 03:34 deep_training/nlp/models/gec_model.py
 -rw-rw-rw-  2.0 fat    10854 b- defN 23-Apr-25 03:34 deep_training/nlp/models/gplinker.py
 -rw-rw-rw-  2.0 fat     3814 b- defN 23-Apr-25 03:34 deep_training/nlp/models/infonce.py
 -rw-rw-rw-  2.0 fat     2459 b- defN 23-Apr-25 03:34 deep_training/nlp/models/mhs_ner.py
 -rw-rw-rw-  2.0 fat     5991 b- defN 23-Apr-25 03:34 deep_training/nlp/models/mhslinker.py
 -rw-rw-rw-  2.0 fat     4661 b- defN 23-Apr-25 03:34 deep_training/nlp/models/onerel_model.py
 -rw-rw-rw-  2.0 fat     2750 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pointer.py
--rw-rw-rw-  2.0 fat    13392 b- defN 23-Apr-25 03:34 deep_training/nlp/models/prefixtuning.py
+-rw-rw-rw-  2.0 fat    13406 b- defN 23-May-19 07:01 deep_training/nlp/models/prefixtuning.py
 -rw-rw-rw-  2.0 fat    15915 b- defN 23-Apr-25 03:34 deep_training/nlp/models/prgc_model.py
 -rw-rw-rw-  2.0 fat    16115 b- defN 23-Apr-25 03:34 deep_training/nlp/models/promptbert_cse.py
 -rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pure_model.py
 -rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-25 03:34 deep_training/nlp/models/simcse.py
 -rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-25 03:34 deep_training/nlp/models/span_ner.py
 -rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-25 03:34 deep_training/nlp/models/spn4re.py
 -rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinker.py
 -rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinkerplus.py
 -rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-25 03:34 deep_training/nlp/models/transformer.py
--rw-rw-rw-  2.0 fat    26238 b- defN 23-May-16 08:28 deep_training/nlp/models/transformer_base.py
+-rw-rw-rw-  2.0 fat    26541 b- defN 23-May-19 07:05 deep_training/nlp/models/transformer_base.py
 -rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tsdae_model.py
 -rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-25 03:34 deep_training/nlp/models/w2ner.py
 -rw-rw-rw-  2.0 fat    16524 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
 -rw-rw-rw-  2.0 fat    19207 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA_parallel/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
 -rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
@@ -159,15 +159,15 @@
 -rw-rw-rw-  2.0 fat    47611 b- defN 23-May-19 04:52 deep_training/nlp/rl/ppo/ppo_trainer.py
 -rw-rw-rw-  2.0 fat       88 b- defN 23-May-15 00:35 deep_training/nlp/rl/rl_base/__init__.py
 -rw-rw-rw-  2.0 fat     3724 b- defN 23-May-15 00:35 deep_training/nlp/rl/rl_base/rl_dataset.py
 -rw-rw-rw-  2.0 fat    10700 b- defN 23-May-15 00:35 deep_training/nlp/rl/utils/__init__.py
 -rw-rw-rw-  2.0 fat     3529 b- defN 23-May-15 00:35 deep_training/nlp/rl/utils/configuration.py
 -rw-rw-rw-  2.0 fat     9844 b- defN 23-May-15 00:35 deep_training/nlp/rl/utils/logging.py
 -rw-rw-rw-  2.0 fat     2868 b- defN 22-Dec-14 08:00 deep_training/nlp/scheduler/__init__.py
--rw-rw-rw-  2.0 fat     7132 b- defN 23-May-19 05:58 deep_training/nlp/utils/__init__.py
+-rw-rw-rw-  2.0 fat     7393 b- defN 23-May-19 07:03 deep_training/nlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat     6323 b- defN 23-Jan-29 01:07 deep_training/nlp/utils/adversarial.py
 -rw-rw-rw-  2.0 fat    15256 b- defN 23-Jan-03 01:54 deep_training/nlp/utils/nlputils.py
 -rw-rw-rw-  2.0 fat      795 b- defN 23-Jan-11 07:02 deep_training/nlp/utils/spearman.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/layers/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/losses/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/metrics/__init__.py
@@ -176,12 +176,12 @@
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
 -rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
 -rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
 -rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
 -rw-rw-rw-  2.0 fat    14500 b- defN 23-May-11 00:39 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      608 b- defN 23-May-19 06:11 deep_training-0.1.5.post4.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-19 06:11 deep_training-0.1.5.post4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-May-19 06:11 deep_training-0.1.5.post4.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    17964 b- defN 23-May-19 06:11 deep_training-0.1.5.post4.dist-info/RECORD
-185 files, 1245707 bytes uncompressed, 316993 bytes compressed:  74.6%
+-rw-rw-rw-  2.0 fat      602 b- defN 23-May-19 07:12 deep_training-0.1.6.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-May-19 07:12 deep_training-0.1.6.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-19 07:12 deep_training-0.1.6.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    17940 b- defN 23-May-19 07:12 deep_training-0.1.6.dist-info/RECORD
+185 files, 1246359 bytes uncompressed, 317020 bytes compressed:  74.6%
```

## zipnote {}

```diff
@@ -537,20 +537,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.5.post4.dist-info/METADATA
+Filename: deep_training-0.1.6.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.5.post4.dist-info/WHEEL
+Filename: deep_training-0.1.6.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.5.post4.dist-info/top_level.txt
+Filename: deep_training-0.1.6.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.5.post4.dist-info/RECORD
+Filename: deep_training-0.1.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,15 +1,15 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.5@post4',
+    version='0.1.6',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['lightning>=2',
```

## deep_training/data_helper/data_helper.py

```diff
@@ -157,17 +157,18 @@
 
         if with_task_params:
             task_specific_params = task_specific_params or {}
             task_params = self.on_task_specific_params()
             if task_params is not None:
                 task_specific_params.update(task_params)
 
-            task_specific_params['learning_rate'] = training_args.learning_rate
-            task_specific_params['learning_rate_for_task'] = training_args.learning_rate_for_task \
-                if training_args.learning_rate_for_task is not None else training_args.learning_rate
+            if training_args is not None:
+                task_specific_params['learning_rate'] = training_args.learning_rate
+                task_specific_params['learning_rate_for_task'] = training_args.learning_rate_for_task \
+                    if training_args.learning_rate_for_task is not None else training_args.learning_rate
 
 
         if hasattr(self.tokenizer,'tokenizer'):
             tokenizer = self.tokenizer
             kwargs_args = {
                 "bos_token_id": tokenizer.bos_token_id,
                 "pad_token_id": tokenizer.pad_token_id,
@@ -254,17 +255,18 @@
 
         if with_task_params:
             task_specific_params = task_specific_params or {}
             task_params = self.on_task_specific_params()
             if task_params is not None:
                 task_specific_params.update(task_params)
 
-            task_specific_params['learning_rate'] = training_args.learning_rate
-            task_specific_params['learning_rate_for_task'] = training_args.learning_rate_for_task \
-                if training_args.learning_rate_for_task is not None else training_args.learning_rate
+            if training_args is not None:
+                task_specific_params['learning_rate'] = training_args.learning_rate
+                task_specific_params['learning_rate_for_task'] = training_args.learning_rate_for_task \
+                    if training_args.learning_rate_for_task is not None else training_args.learning_rate
 
         kwargs_args = {
             "bos_token_id": tokenizer.bos_token_id,
             "pad_token_id": tokenizer.pad_token_id,
             "eos_token_id": tokenizer.eos_token_id,
             "sep_token_id": tokenizer.sep_token_id,
             "return_dict": return_dict,
```

## deep_training/nlp/models/prefixtuning.py

```diff
@@ -13,29 +13,29 @@
 from ...data_helper import PrefixModelArguments
 from .transformer import TransformerModel
 from ..layers.crf import CRF
 from ..layers.prefix_encoder import PrefixEncoder
 from ..layers.seq_pointer import EfficientPointerLayer, PointerLayer, f1_metric_for_pointer
 from ..losses.loss_globalpointer import loss_for_pointer
 from ..metrics.pointer import metric_for_pointer
-from ..utils import get_value_from_args
+from ..utils import get_value_from_args_assert
 
 __all__ = [
     'PrefixTransformerForModel',
     'PrefixTransformerForSequenceClassification',
     'PrefixTransformerForTokenClassification',
     'PrefixTransformerForCRF'
 ]
 
 
 
 
 class PrefixTransformerForModel(TransformerModel):
     def __init__(self, *args: Any, **kwargs: Any):
-        prompt_args = get_value_from_args('prompt_args', PrefixModelArguments, *args, **kwargs)
+        prompt_args = get_value_from_args_assert('prompt_args', PrefixModelArguments, *args, **kwargs)
         super().__init__(*args, **kwargs)
         self.prompt_args = prompt_args
         config = self.config
         config.pre_seq_len = prompt_args.pre_seq_len
         if prompt_args.prompt_type != 0:
             config.prefix_projection = prompt_args.prefix_projection
             config.prefix_hidden_size = prompt_args.prefix_hidden_size
```

## deep_training/nlp/models/transformer_base.py

```diff
@@ -11,15 +11,15 @@
 import torch
 from torch import nn, Tensor
 from transformers import (
     PretrainedConfig,
 )
 
 
-from ..utils import configure_optimizers, get_value_from_args
+from ..utils import configure_optimizers, get_value_from_args_assert, get_value_from_args
 from ..utils.adversarial import AdversarialMethods
 from ...data_helper import TrainingArguments, ModelArguments, PrefixModelArguments, DataArguments
 
 
 # class TransformerMeta(type):
 #     def __new__(cls, name, base, attr,*args,**kwargs):
 #         alter = tuple(b for b in base if issubclass(b,TransformerBase))
@@ -99,15 +99,15 @@
             cls_.__BACKBONE_CLASS__ = backbone_class
         return cls_
 
 
 
 class TransformerBase(MyLightningModule,metaclass=TransformerFakeMeta):
     def __init__(self,*args,**kwargs):
-        config = get_value_from_args('config',PretrainedConfig,*args,**kwargs)
+        config = get_value_from_args_assert('config',PretrainedConfig,*args,**kwargs)
         super(TransformerBase, self).__init__()
         self.config = config
         self.base_model_prefix = None
         self.config_class = None
         self._trainer:  typing.Optional["pl.Trainer"]  = None
 
     def forward(self, *args, **batch):
@@ -152,16 +152,16 @@
 
     @property
     def min_steps(self) -> int:
         return self.trainer.min_steps if self._trainer else 0
 
 
     def from_pretrained(self,CLS, *args, **kwargs):
-        config = get_value_from_args('config', PretrainedConfig, *args, **kwargs)
-        model_args = get_value_from_args('model_args', ModelArguments, *args, **kwargs)
+        config = get_value_from_args_assert('config', PretrainedConfig, *args, **kwargs)
+        model_args = get_value_from_args_assert('model_args', ModelArguments, *args, **kwargs)
 
         if model_args.model_name_or_path:
             args_new = tuple(v for v in args
                              if not isinstance(v, ModelArguments) and \
                              not isinstance(v, TrainingArguments) and \
                              not isinstance(v,PretrainedConfig) and \
                              not isinstance(v,PrefixModelArguments) and \
@@ -230,29 +230,31 @@
         # return [(self.model if self.base_model_prefix is not None else self , lr), ]
         return [(self, lr), ]
 
 
 
 class TransformerLightningModule(MyLightningModule):
     def __init__(self, *args,**kwargs):
-        config = get_value_from_args('config',PretrainedConfig,*args,**kwargs)
-        model_args = get_value_from_args('model_args', ModelArguments, *args, **kwargs)
+        config = get_value_from_args_assert('config',PretrainedConfig,*args,**kwargs)
+        model_args = get_value_from_args_assert('model_args', ModelArguments, *args, **kwargs)
         training_args = get_value_from_args('training_args', TrainingArguments, *args, **kwargs)
         super(TransformerLightningModule, self).__init__()
         if not hasattr(config, 'task_specific_params') or config.task_specific_params is None:
             config.task_specific_params = {}
         task_specific_params = config.task_specific_params
-        task_specific_params['learning_rate'] = training_args.learning_rate
-        task_specific_params['learning_rate_for_task'] = training_args.learning_rate_for_task \
-            if training_args.learning_rate_for_task is not None else training_args.learning_rate
+        if training_args is not None:
+            task_specific_params['learning_rate'] = training_args.learning_rate
+            task_specific_params['learning_rate_for_task'] = training_args.learning_rate_for_task \
+                if training_args.learning_rate_for_task is not None else training_args.learning_rate
+
+            if training_args.adv is not None and training_args.adv['mode'] != None:
+                assert training_args.adv['mode']  in AdversarialMethods.keys(), ValueError('no support adv mode {} , must be in {}'.format(training_args.adv['mode'],','.join(AdversarialMethods.keys())))
+                self.automatic_optimization = False
         print(training_args)
         print(model_args)
-        if training_args.adv is not None and training_args.adv['mode'] != None:
-            assert training_args.adv['mode']  in AdversarialMethods.keys(), ValueError('no support adv mode {} , must be in {}'.format(training_args.adv['mode'],','.join(AdversarialMethods.keys())))
-            self.automatic_optimization = False
 
         try:
             self.save_hyperparameters(ignore=['config'])
         except:
             pass
         self.config = config
         self.model_args = model_args
@@ -261,53 +263,56 @@
         if hasattr(self,'__BACKBONE_CLASS__') and len(self.__BACKBONE_CLASS__) > 0:
             self.set_model(self.__BACKBONE_CLASS__[0](*args, **kwargs))
 
 
         self.training_step_fn = self.training_step
         self.embeddings_forward_fn = None
 
-        #对抗训练
-        if training_args.adv is not None and training_args.adv['mode'] is not None:
-            self.embeddings_forward_fn = self.get_embeddings_module().embeddings.forward
-
-            self.training_step = self.adv_training_step
-            if training_args.adv['mode'].find('local') != -1:
-                self.adversarial = AdversarialMethods[training_args.adv['mode']](model=self.model)
+
+        if training_args is not None:
+            self.gradient_clip_val = training_args.max_grad_norm
+            #对抗训练
+            if training_args.adv is not None and training_args.adv['mode'] is not None:
+                self.embeddings_forward_fn = self.get_embeddings_module().embeddings.forward
+
+                self.training_step = self.adv_training_step
+                if training_args.adv['mode'].find('local') != -1:
+                    self.adversarial = AdversarialMethods[training_args.adv['mode']](model=self.model)
+                else:
+                    self.adversarial = AdversarialMethods[training_args.adv['mode']](model=self.model,
+                                                                                     emb_name=training_args.adv.get('emb_name', 'embedding'))
+
+                k = 'lightning.pytorch.trainer.configuration_validator'
+                if k in sys.modules:
+                    setattr( sys.modules[k],'__verify_manual_optimization_support' , verify_manual_optimization_support)
             else:
-                self.adversarial = AdversarialMethods[training_args.adv['mode']](model=self.model,
-                                                                                 emb_name=training_args.adv.get('emb_name', 'embedding'))
+                self.adversarial = None
 
-            k = 'lightning.pytorch.trainer.configuration_validator'
-            if k in sys.modules:
-                setattr( sys.modules[k],'__verify_manual_optimization_support' , verify_manual_optimization_support)
-        else:
-            self.adversarial = None
 
-        self.gradient_clip_val = training_args.max_grad_norm
 
-        if training_args.hierarchical_position is not None and (training_args.hierarchical_position > 0 and training_args.hierarchical_position < 1):
-            #绝对位置编码 分层位置编码
-            def forward(cls,input: Tensor) -> Tensor:
-                # return F.embedding(
-                #     input, self.weight, self.padding_idx, self.max_norm,
-                #     self.norm_type, self.scale_grad_by_freq, self.sparse)
-                position_ids = input
-                alpha = training_args.hierarchical_position
-                embeddings = cls.weight - alpha * cls.weight[:1]
-                embeddings = embeddings / (1 - alpha)
-                x_idx = position_ids // cls.num_embeddings
-                y_idx = position_ids % cls.num_embeddings
-
-                embeddings_x = torch.index_select(embeddings,dim=0,index=x_idx.view(-1))
-                embeddings_y = torch.index_select(embeddings,dim=0,index=y_idx.view(-1))
-                embeddings = alpha * embeddings_x + (1 - alpha) * embeddings_y
-                return embeddings
+            if training_args.hierarchical_position is not None and (training_args.hierarchical_position > 0 and training_args.hierarchical_position < 1):
+                #绝对位置编码 分层位置编码
+                def forward(cls,input: Tensor) -> Tensor:
+                    # return F.embedding(
+                    #     input, self.weight, self.padding_idx, self.max_norm,
+                    #     self.norm_type, self.scale_grad_by_freq, self.sparse)
+                    position_ids = input
+                    alpha = training_args.hierarchical_position
+                    embeddings = cls.weight - alpha * cls.weight[:1]
+                    embeddings = embeddings / (1 - alpha)
+                    x_idx = position_ids // cls.num_embeddings
+                    y_idx = position_ids % cls.num_embeddings
+
+                    embeddings_x = torch.index_select(embeddings,dim=0,index=x_idx.view(-1))
+                    embeddings_y = torch.index_select(embeddings,dim=0,index=y_idx.view(-1))
+                    embeddings = alpha * embeddings_x + (1 - alpha) * embeddings_y
+                    return embeddings
 
-            position_embeddings = self.get_embeddings_module().embeddings.position_embeddings
-            position_embeddings.forward = partial(forward,position_embeddings)
+                position_embeddings = self.get_embeddings_module().embeddings.position_embeddings
+                position_embeddings.forward = partial(forward,position_embeddings)
 
 
     def get_embeddings_module(self):
         base_model_prefix = self.backbone.base_model_prefix
         current_model = self.backbone.model
         tmp_obj = current_model
         while tmp_obj is not None:
```

## deep_training/nlp/utils/__init__.py

```diff
@@ -140,16 +140,25 @@
     def __new__(cls, name,bases,attr,*args,**kwargs):
         excepts = kwargs.pop('except',None)
         return super(ExceptClassMeta, cls).__new__(cls, name,tuple(_ for _ in bases if not str(_).endswith('__.{}\'>'.format(excepts))) if excepts is not None else bases,attr)
 
 class ExceptCLASS(metaclass=ExceptClassMeta):...
 
 
-def get_value_from_args(key,dtype,*args,**kwargs):
+def get_value_from_args_assert(key,dtype,*args,**kwargs):
     value = kwargs.get(key, None)
     if value is not None:
         for item in args:
             if isinstance(item,dtype):
                 value = item
                 break
     assert value is not None, ValueError('no param ',key)
+    return value
+
+def get_value_from_args(key,dtype,*args,**kwargs):
+    value = kwargs.get(key, None)
+    if value is not None:
+        for item in args:
+            if isinstance(item,dtype):
+                value = item
+                break
     return value
```

## Comparing `deep_training-0.1.5.post4.dist-info/METADATA` & `deep_training-0.1.6.dist-info/METADATA`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deep-training
-Version: 0.1.5-post4
+Version: 0.1.6
 Summary: an easy training architecture
 Home-page: https://github.com/ssbuild/deep_training
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache License 2.0
 Platform: UNKNOWN
 Requires-Dist: lightning (>=2)
```

## Comparing `deep_training-0.1.5.post4.dist-info/RECORD` & `deep_training-0.1.6.dist-info/RECORD`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=_TVzKmZ7fL7gjY0AoOOvnQcgx-jO5-IgEOUxIlkZGD0,903
+deep_training/setup.py,sha256=-ReUbDEwdu2vJ1w0n3LTKaimbADwtb2jZwjwgJgPeKI,897
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
-deep_training/data_helper/data_helper.py,sha256=Pe45VeRSRl7izzI5TVfsZ6PnzGK3tyfGWMxGsoclnLs,17724
+deep_training/data_helper/data_helper.py,sha256=EPvD6_eK0Ju2SHySXfXRgQ3I2ojkR3CLqf8LR1NUdWs,17834
 deep_training/data_helper/data_module.py,sha256=EmXCTU2jLnldgHubQL4lpwzmlJErSVJf7YIotbQBQJU,5041
 deep_training/data_helper/training_args.py,sha256=XGUXdty0SE6n8xqk6J0lySFvaYSGMVo2zuq6paFQ8sM,12121
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
 deep_training/nlp/layers/activate.py,sha256=0q7htFl9Az2fdUjrjv-QMUCE5oenYPVTLZ3lRemIKzA,241
 deep_training/nlp/layers/crf.py,sha256=JTihPuJuBBp83I9UZzVg0wogwwpdJrs0VKtuLPBSCDM,13271
 deep_training/nlp/layers/handshakingkernel.py,sha256=BRJZbEjKM347q8zEMEtJXxXjmqhegmQgqebhqMy4UkI,4653
@@ -79,25 +79,25 @@
 deep_training/nlp/models/gec_model.py,sha256=YLJMX0e4UMe2TRakOmUjNGsKBmHv_WRCteWWYxqPBVw,4209
 deep_training/nlp/models/gplinker.py,sha256=3RjkwbcCw8RSBK5nYz0XdUz-f4dMj0M8vtCRt5D71MM,10854
 deep_training/nlp/models/infonce.py,sha256=8ytOu5iUuEj0j4wa8zRX9t6uB-dCRuhTp6C8nh1d5eQ,3814
 deep_training/nlp/models/mhs_ner.py,sha256=czbhCnnAZFZBuGRO6mLPGhQPjSU1EXOqekNeU0vB6_I,2459
 deep_training/nlp/models/mhslinker.py,sha256=jI1sppDEqqXqmvX3kiWyivCwu1tsGfTF4HuWykFfzik,5991
 deep_training/nlp/models/onerel_model.py,sha256=uy7DObzLO33bf8C_M8f0jvCofk5qCWtr_e-yWoIfum8,4661
 deep_training/nlp/models/pointer.py,sha256=VY6zkd314vIhKp5xqMU0Kh402T157jBrxJjItzkkry0,2750
-deep_training/nlp/models/prefixtuning.py,sha256=NiwMtdnFRazsKNhACB94kBDNYV0MK5Y7_aPDYthIcvQ,13392
+deep_training/nlp/models/prefixtuning.py,sha256=Hr6EgfW6NJx9e9BGyqscB0ay23nMs3oNhm1PTCShdeM,13406
 deep_training/nlp/models/prgc_model.py,sha256=KHiwjM5ay2xPUPjFYV0-lVH4diSxVuLv1gLJCIHM3Qk,15915
 deep_training/nlp/models/promptbert_cse.py,sha256=VsoutrF8VFNwmrhJnA4b6FcEejq-MP1L_5BI1q3l0ok,16115
 deep_training/nlp/models/pure_model.py,sha256=LD8cYvvRirnP8iMFCyRhsNXRHpZt93Kh2WGoUZoEnFw,5149
 deep_training/nlp/models/simcse.py,sha256=ubVGkeMatDeIUqySV8Tc2TJHvaRKb4p3JOvUTOOhaRo,3949
 deep_training/nlp/models/span_ner.py,sha256=rD0TY2K-zesfRFGfDguqkSAfxHGgbQHG3K5QZ6gc7Zg,6022
 deep_training/nlp/models/spn4re.py,sha256=g_pk3bNqpH41EnzJ77oWPsKvbUwzJfaBYMux5FiEc60,14454
 deep_training/nlp/models/tplinker.py,sha256=PJ9smipeIiA1CDi8xz1gIg2DBWzO5C1B2wITItEsd1A,11383
 deep_training/nlp/models/tplinkerplus.py,sha256=hP4KD3rf2hktfQzHnI7RA2j_2cjk_0G5v6CkbLt1gvQ,8157
 deep_training/nlp/models/transformer.py,sha256=ZuywgLt3HZhsR4sJ4SyZvrYgaVJW8lCn5JH1j_IceXE,6624
-deep_training/nlp/models/transformer_base.py,sha256=vxDVNUxRbSMpnLLxwtqS4aOmI_Teg5ggSY-KVHRuCEE,26238
+deep_training/nlp/models/transformer_base.py,sha256=HUnSGXV3-f3ce2Mn8uEJUi-amusmxeXvuBHTT8WkAFU,26541
 deep_training/nlp/models/tsdae_model.py,sha256=lb04RIGkhHhilD-vdkfb8YK9hnTck1nN79WX1Pngbbk,7968
 deep_training/nlp/models/w2ner.py,sha256=z0BortOquZSzmma355wNLz1ofLku_hMb2CjL4KDf-PM,9040
 deep_training/nlp/models/LLaMA/__init__.py,sha256=asn9Wxkl4lG12dRVgxq7Lz4BGCNDaRFL155haVMDNso,16524
 deep_training/nlp/models/LLaMA/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/LLaMA_parallel/__init__.py,sha256=4fOhbq0tQOTSH5e3X6XN3PnI6athUR8tsTCn4AUg94Q,19207
 deep_training/nlp/models/LLaMA_parallel/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/PaLM/__init__.py,sha256=P1qwWPUycRmZ6I48tov6janJUNpp4L-iMoVN54ykcQw,31627
@@ -158,15 +158,15 @@
 deep_training/nlp/rl/ppo/ppo_trainer.py,sha256=SO5nUGHLJLoITzNQOHv-wRIX1Zq0e5zKV3wc9-udN1I,47611
 deep_training/nlp/rl/rl_base/__init__.py,sha256=6pBQ9y-xnuMFThlwlzpT1oCVLZJG0rDUvWvFwu0ox3Y,88
 deep_training/nlp/rl/rl_base/rl_dataset.py,sha256=dSFnBt8u1SddRYX3DThJx2tRxISpd4xQSlJFQ80YPlA,3724
 deep_training/nlp/rl/utils/__init__.py,sha256=RzPjehgDE4v_PEokLSdhIxD2yObNtjXx8i_ZdAvfjUY,10700
 deep_training/nlp/rl/utils/configuration.py,sha256=X-ACQ-UJ5nL24ivY0yyKfnWCxU0Qng_GTZzjHuzK7GU,3529
 deep_training/nlp/rl/utils/logging.py,sha256=mF0eKsv9BqBNymZt_wnkMP04AF8N9B_Lhg5Wpb9iRmc,9844
 deep_training/nlp/scheduler/__init__.py,sha256=-zaiinwJzOBWypkNodSZO12kqbswVsPy5JCsYpvLbbY,2868
-deep_training/nlp/utils/__init__.py,sha256=NfhdZNmz0SE5AerELj7oTxAPcJKhPUGCLAgkBKuVFPs,7132
+deep_training/nlp/utils/__init__.py,sha256=4KXIbHGRwYv5iWH1ZjZ6vjY4fWDtCWurQVki2_baOIU,7393
 deep_training/nlp/utils/adversarial.py,sha256=FNZlg8mV23YXRu7aDcu1JZBUGBV01hi_bwRzfFyzEzM,6323
 deep_training/nlp/utils/nlputils.py,sha256=KEmFliU1IqJHy3INNDvOriEMlBkP8GNwe8Y8_c_imZQ,15256
 deep_training/nlp/utils/spearman.py,sha256=tOpaah5bt_65ferL_uI6FMfKvNexi7CQztSYLj-k3yo,795
 deep_training/tfnlp/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
 deep_training/tfnlp/layers/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
 deep_training/tfnlp/losses/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
 deep_training/tfnlp/metrics/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
@@ -175,11 +175,11 @@
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=F1usofzi1lBVHeieDJ7WWdfd1d0Q7tftktwdJgczlg8,14500
-deep_training-0.1.5.post4.dist-info/METADATA,sha256=u9V49HjxaFSO_UTYceKMWKYczzqfX8k2pc8JpyhKBnY,608
-deep_training-0.1.5.post4.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-deep_training-0.1.5.post4.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.5.post4.dist-info/RECORD,,
+deep_training-0.1.6.dist-info/METADATA,sha256=jpsnD9wEUM0Jyzn5Q-rmqBw_8uGphkyTrLFGgrw2_8c,602
+deep_training-0.1.6.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.6.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.6.dist-info/RECORD,,
```

